{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is testing code for writing python implementations of HMMs wiht cuda GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#activate venv\n",
    "!source ../.venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claude 3.5 Sonnet implementation of an HMM\n",
    "Currently fails miserably!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class HiddenMarkovModel:\n",
    "    def __init__(self, n_states, n_observations):\n",
    "        \"\"\"\n",
    "        Initialize a Hidden Markov Model.\n",
    "        \n",
    "        Args:\n",
    "            n_states: Number of hidden states\n",
    "            n_observations: Number of possible observations\n",
    "        \"\"\"\n",
    "        # Initialize parameters with random values\n",
    "        self.n_states = n_states\n",
    "        self.n_observations = n_observations\n",
    "        \n",
    "        # Transition probabilities (A matrix)\n",
    "        self.A = np.random.random((n_states, n_states))\n",
    "        self.A = self.A / self.A.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        # Emission probabilities (B matrix)\n",
    "        self.B = np.random.random((n_states, n_observations))\n",
    "        self.B = self.B / self.B.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        # Initial state probabilities (Ï€ vector)\n",
    "        self.pi = np.random.random(n_states)\n",
    "        self.pi = self.pi / self.pi.sum()\n",
    "\n",
    "    def forward(self, observations):\n",
    "        \"\"\"\n",
    "        Compute forward probabilities for a sequence of observations.\n",
    "        \n",
    "        Args:\n",
    "            observations: Array-like sequence of observation indices\n",
    "            \n",
    "        Returns:\n",
    "            alpha: Forward probabilities matrix\n",
    "            likelihood: Probability of the observation sequence\n",
    "        \"\"\"\n",
    "        observations = np.array(observations)\n",
    "        T = len(observations)\n",
    "        alpha = np.zeros((T, self.n_states))\n",
    "        \n",
    "        # Initialize first step\n",
    "        alpha[0] = self.pi * self.B[:, observations[0]]\n",
    "        \n",
    "        # Forward recursion\n",
    "        for t in range(1, T):\n",
    "            for j in range(self.n_states):\n",
    "                alpha[t, j] = self.B[j, observations[t]] * np.sum(alpha[t-1] * self.A[:, j])\n",
    "        \n",
    "        likelihood = np.sum(alpha[-1])\n",
    "        return alpha, likelihood\n",
    "\n",
    "    def backward(self, observations):\n",
    "        \"\"\"\n",
    "        Compute backward probabilities for a sequence of observations.\n",
    "        \n",
    "        Args:\n",
    "            observations: Array-like sequence of observation indices\n",
    "            \n",
    "        Returns:\n",
    "            beta: Backward probabilities matrix\n",
    "        \"\"\"\n",
    "        observations = np.array(observations)\n",
    "        T = len(observations)\n",
    "        beta = np.zeros((T, self.n_states))\n",
    "        \n",
    "        # Initialize last step\n",
    "        beta[-1] = 1\n",
    "        \n",
    "        # Backward recursion\n",
    "        for t in range(T-2, -1, -1):\n",
    "            for i in range(self.n_states):\n",
    "                beta[t, i] = np.sum(self.A[i, :] * self.B[:, observations[t+1]] * beta[t+1])\n",
    "        \n",
    "        return beta\n",
    "\n",
    "    def viterbi(self, observations):\n",
    "        \"\"\"\n",
    "        Find most likely state sequence using Viterbi algorithm.\n",
    "        \n",
    "        Args:\n",
    "            observations: Array-like sequence of observation indices\n",
    "            \n",
    "        Returns:\n",
    "            path: Most likely state sequence\n",
    "            probability: Probability of the most likely path\n",
    "        \"\"\"\n",
    "        observations = np.array(observations)\n",
    "        T = len(observations)\n",
    "        viterbi = np.zeros((T, self.n_states))\n",
    "        backpointer = np.zeros((T, self.n_states), dtype=int)\n",
    "        \n",
    "        # Initialize first step\n",
    "        viterbi[0] = np.log(self.pi) + np.log(self.B[:, observations[0]])\n",
    "        \n",
    "        # Recursion\n",
    "        for t in range(1, T):\n",
    "            for j in range(self.n_states):\n",
    "                temp = viterbi[t-1] + np.log(self.A[:, j])\n",
    "                viterbi[t, j] = np.max(temp) + np.log(self.B[j, observations[t]])\n",
    "                backpointer[t, j] = np.argmax(temp)\n",
    "        \n",
    "        # Backtrack\n",
    "        path = [np.argmax(viterbi[-1])]\n",
    "        for t in range(T-1, 0, -1):\n",
    "            path.append(backpointer[t, path[-1]])\n",
    "        path.reverse()\n",
    "        \n",
    "        probability = np.exp(np.max(viterbi[-1]))\n",
    "        return path, probability\n",
    "\n",
    "    def baum_welch(self, observations, max_iter=100, tol=1e-6):\n",
    "        \"\"\"\n",
    "        Estimate HMM parameters using Baum-Welch algorithm.\n",
    "        \n",
    "        Args:\n",
    "            observations: Array-like sequence of observation indices\n",
    "            max_iter: Maximum number of iterations\n",
    "            tol: Convergence tolerance\n",
    "            \n",
    "        Returns:\n",
    "            likelihood: Final log-likelihood\n",
    "        \"\"\"\n",
    "        observations = np.array(observations)\n",
    "        T = len(observations)\n",
    "        prev_likelihood = float('-inf')\n",
    "        \n",
    "        for _ in range(max_iter):\n",
    "            # E-step\n",
    "            alpha, likelihood = self.forward(observations)\n",
    "            beta = self.backward(observations)\n",
    "            \n",
    "            # Compute gammas and xis\n",
    "            gamma = alpha * beta / likelihood\n",
    "            xi = np.zeros((T-1, self.n_states, self.n_states))\n",
    "            \n",
    "            for t in range(T-1):\n",
    "                for i in range(self.n_states):\n",
    "                    for j in range(self.n_states):\n",
    "                        xi[t, i, j] = (alpha[t, i] * self.A[i, j] * \n",
    "                                     self.B[j, observations[t+1]] * beta[t+1, j])\n",
    "                xi[t] = xi[t] / xi[t].sum()\n",
    "            \n",
    "            # M-step\n",
    "            # Update initial probabilities\n",
    "            self.pi = gamma[0]\n",
    "            \n",
    "            # Update transition probabilities\n",
    "            self.A = xi.sum(axis=0) / gamma[:-1].sum(axis=0).reshape(-1, 1)\n",
    "            \n",
    "            # Update emission probabilities\n",
    "            for j in range(self.n_states):\n",
    "                for k in range(self.n_observations):\n",
    "                    mask = observations == k\n",
    "                    self.B[j, k] = gamma[mask, j].sum() / gamma[:, j].sum()\n",
    "            \n",
    "            # Check convergence\n",
    "            if abs(likelihood - prev_likelihood) < tol:\n",
    "                break\n",
    "            prev_likelihood = likelihood\n",
    "            \n",
    "        return likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most likely state sequence: [np.int64(1), np.int64(2), np.int64(2), np.int64(0), np.int64(1)]\n",
      "Probability: 1.2187241269992641e-05\n",
      "Final log-likelihood: 0.2499999999993308\n"
     ]
    }
   ],
   "source": [
    "# Create a simple HMM with 2 states and 3 possible observations\n",
    "hmm = HiddenMarkovModel(n_states=3, n_observations=5)\n",
    "    \n",
    "# Generate some sample observations\n",
    "observations = [0, 1, 2, 1, 0]\n",
    "    \n",
    "# Find most likely state sequence\n",
    "states, probability = hmm.viterbi(observations)\n",
    "print(f\"Most likely state sequence: {states}\")\n",
    "print(f\"Probability: {probability}\")\n",
    "    \n",
    "# Estimate parameters using Baum-Welch\n",
    "likelihood = hmm.baum_welch(observations)\n",
    "print(f\"Final log-likelihood: {likelihood}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
